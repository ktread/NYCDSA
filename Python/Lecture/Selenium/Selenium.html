<!DOCTYPE html>
<html>
  <head>
    <title>Jump Start: Selenium</title>
    <meta charset="utf-8">
    <meta name="author" content="NYC Data Science Academy" />
    <link rel="stylesheet" href="asset/css/footer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Jump Start: Selenium
### NYC Data Science Academy

---




## Prerequisites
#### Step #1
- Windows users: download the **latest** chromedriver from [here](https://chromedriver.storage.googleapis.com/index.html)
 - Unzip it under any folder you like (preferably with short file path)

- Mac users:
 - Install homebrew: http://brew.sh/
 
 - Run `brew tap caskroom/cask` and `brew cask install chromedriver` on the terminal
 

---
## Prerequisites
#### Step #2
- Windows users: open Anaconda prompt and cd to the lecture folder. 
 - `pip install -r requirements.txt`

- Mac users: open Terminal and cd to the lecture folder. 
 - `pip install -r requirements.txt`

- What is requirements.txt?
 - It lists all the required packages of the project you are developing and makes your work reproducible.
 
- How to generate the requirement file?
 - `pip freeze | grep selenium` to get the version of one package.
 - `pip freeze &gt; requirements.txt` to save the version of all the packages in the `requirements.txt` file

---
## When should we use Selenium
- We want to scrape all the reviews of [samsung galaxy s8](https://www.verizonwireless.com/smartphones/samsung-galaxy-s8/)
 
- You can find all the reviews at the bottom of the page and you need to click the Next button to go to the next page. 

- However, the url stays the same. It usually means this website uses [Ajax](https://en.wikipedia.org/wiki/Ajax_(programming), you should consider using Seleium.

- Another important sign is when you get an empty list from scrapy shell and you are pretty sure the xpath you typed is correct. This usually means that part of data is retrieved by making Ajax calls. 

---
## General Workflow
- In `verizon_starter.py` file, we first initialize a Chrome browser:
```python
driver = webdriver.Chrome()
```
- You will see our script opens a new Chrome browser if you run `python verizon_starter.py` on the terminal/Anaconda Prompt.
 
- Windows users need to specify the path to chrome driver you just downloaded.
```python
driver = webdriver.Chrome(r'path\to\the\chromedriver.exe')
```
- The letter `r` stands for raw string so `\t` will not be interpreted as tab.
 
- Go to the page that we want to scrape:
```python
driver.get(url_you_want_to_scrape)
```

---
## General Workflow

- Use different locating methods from Selenium to find the element or elements you are looking for.
 - Most commonly used ones are `find_element_by_xpath` and `find_elements_by_xpath`
 - The first one returns a single element (tag) and the second one returns a list of elements (tags).
 
 - Check more details on the documentation [here](http://selenium-python.readthedocs.io/locating-elements.html)

- To test your xpath, you can print the length of the list or the value of the element on the terminal.

- Once you locate the element, you can use `element.text` to return the text.

- To get the attribute instead of the text of each element, use `element.get_attribute()`

- Initialize an empty dictionary to save each review.

---
## General Workflow

- Locate the next button element on the page and then call `button.click()` to actually click it.

- The script starts working but sometimes you might get an error saying that some element is not available on the page. 

- This is because our code runs so fast and the new reviews or the button on the next page is not displayed yet due to the the lag of the Internet.

- We can add `time.sleep(2)` at the end of the while loop to give the browser a 2 seconds buffer before we start scraping the next page.

---
## Explicit Waits

- In the `verizon_final_csv.py` file, we save the scraped data to a local csv file.

- We also use a more efficient way to handle the waiting called [Explicit Waits](http://selenium-python.readthedocs.io/waits.html?).

- Instead of always implicitly waiting a certain amount of time, we wait until the elements become available or clickable.

---

## Explicit Waits

- Tell the driver (browser) to wait at most 10 seconds.
```python
wait_review = WebDriverWait(driver, 10)
```
```python
wait_button = WebDriverWait(driver, 10)
```

- Wait for all the reviews to be displayed on the page.
```python
wait_review.until(EC.presence_of_all_elements_located((...)))
```
 
- Wait for the next button to be clickable.
```python
wait_button.until(EC.element_to_be_clickable(()))
```
---
## Common tricks using Selenium
- [Login a website](https://stackoverflow.com/a/21186465)

- [Infinite Scroll](https://stackoverflow.com/a/52154646)

- [Go Incognito Mode](https://stackoverflow.com/a/27630230)

- [Open a new tab](https://stackoverflow.com/a/28432939)

- [Switch between tabs](https://stackoverflow.com/a/28716311)

- [Switch between windows](https://stackoverflow.com/a/39037983)

---
## Saving data to database

- In the `verizon_final_db.py` file, we give an example how to save the scraped data to a MySQL database.

- We use a package called [peewee](http://peewee.readthedocs.io/en/latest/peewee/quickstart.html) to help us connect to the remote database. 

- Windows users:
 - Go [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/) and search for peewee.
 - Download the cp version that matches your python installation.
 - Go to the directory where you downloaded the file and `pip install FILENAME` (you can just type a few chars and tab). 
 - Or move the file to your users (where Anaconda will default to) and just type `pip install FILENAME` from there.

- Mac users: open Terminal and switch to python3 environment.
 - `pip install peewee`

---
## Saving data to database

- We also use an industry standard method called configure file to save all the configurations of our database.

- You need to replace the username and password with yours in the `conf.ini` file.

- **When you push the project to Github, remember to put all the configuration files in the `gitignore` file, especially the ones that have password.**

- In the `conf.ini` file, we have a section called `database` and assign all the values using a dictionary like notation. Check more example from the [documentation](https://docs.python.org/3/library/configparser.html)
```python
[database]
db_name = your_username_db
db_port = your_port_number
db_host = your_ip_address
user = your_username
passwd = your_password
```

---
## Saving data to database

- In the `db.py` file, we use a ConfigParser to get all the settings.

```python
cfg = ConfigParser(interpolation=None)
cfg.read('conf.ini')
db_conf = cfg['database']
db_name = db_conf['db_name']
db_port = db_conf['db_port']
db_host = db_conf['db_host']
user = db_conf['user']
passwd = db_conf['passwd']
```

- After getting the settings from ConfigParser, we initialize a MySQL databse object using `peewee` and then connect it.

```python
myDB = MySQLDatabase(db_name, host=db_host, 
       port=int(db_port), user=user, passwd=passwd)
myDB.connect()
```

---
## Saving data to database

- Then we create a `Reivew` class to represent the Review table we want to create in our database.
```python
  class Review(Model):
	  username = TextField()
	  title = TextField()
	  text = TextField()
	  date_published = DateTimeField()
	  rating = IntegerField()

	  class Meta:
		  database = myDB
```

- In peewee, class and field have their corresponding meaning in MySQL database.

|Thing|	Corresponds toâ€¦|
| ------------- |:-------------:|
|Model class|	Database table|
|Field instance|	Column on a table|
|Model instance|	Row in a database table|

---
## Saving data to database

- The best way to understand how the Model and field works in peewee is to read the [documentation](http://peewee.readthedocs.io/en/latest/peewee/models.html#models-and-fields).

- We create a separate Review object for each review at the end of the while loop and save it as one row of our table.
```python
review = Review(username=username,
				title=title,
				text=text,
				date_published=date_published,
				rating=rating)
review.save()
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('showSlide', function (slide) {setTimeout(function() {window.dispatchEvent(new Event('resize'));}, 100)});</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js"></script>
<script>
(function () {
  $('.remark-slide-content').prepend('<div class="nyc-header" />');
})();
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
