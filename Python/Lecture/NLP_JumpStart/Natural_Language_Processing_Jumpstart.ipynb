{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JumpStart: Natural  Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** A general introduction to basic NLP methods to serve as a foundation for further self study and additional NLP curriculums. This notebook intends to serve as a basis for various techniques such as text processing, information retrieval, and classifying text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Reddit dataset we will use today comes from Google BigQuery. You can find it [here](https://bigquery.cloud.google.com/table/fh-bigquery:reddit_posts.2018_05?pli=1).\n",
    " - The data is public but you need to have an active account on Google Clound Platform first in order to access it.\n",
    "- The original data was huge so we sampled it from the top 10 subreddit.\n",
    "\n",
    "- We will also learn the following NLP packages in Python along the way\n",
    "\n",
    " - [NLTK](http://www.nltk.org/) - a very popular package for doing NLP in Python\n",
    "\n",
    " - [Textblob](https://textblob.readthedocs.io/en/dev/) - similar to NLTK but provides a higher level API for easy accessing.\n",
    "\n",
    " - [WordCloud](https://github.com/amueller/word_cloud) - how to run wordcloud in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Open your **Terminal/Anaconda Prompt**, cd to the lecture code folder and run the following command:\n",
    " - `pip install -r requirements.txt`\n",
    "\n",
    "- After installing all the required packages, run the following command:\n",
    " - `python -m textblob.download_corpora`\n",
    " \n",
    "- Restart this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ktread/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/ktread/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Uncomment the following line the first time you run the code\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://s3.amazonaws.com/nycdsabt01/reddit_top10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is always a good idea to check the shape of the dataframe and column types before you run any type of operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737339, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc      int64\n",
       "subreddit       object\n",
       "author          object\n",
       "domain          object\n",
       "url             object\n",
       "num_comments     int64\n",
       "score            int64\n",
       "ups              int64\n",
       "downs            int64\n",
       "title           object\n",
       "selftext        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.373390e+05</td>\n",
       "      <td>737339.000000</td>\n",
       "      <td>737339.000000</td>\n",
       "      <td>737339.000000</td>\n",
       "      <td>737339.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.526486e+09</td>\n",
       "      <td>6.935000</td>\n",
       "      <td>52.798143</td>\n",
       "      <td>52.798143</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.342795e+05</td>\n",
       "      <td>151.224431</td>\n",
       "      <td>721.943659</td>\n",
       "      <td>721.943659</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.525306e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.525723e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.526487e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.527045e+09</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.527725e+09</td>\n",
       "      <td>46164.000000</td>\n",
       "      <td>100389.000000</td>\n",
       "      <td>100389.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc   num_comments          score            ups     downs\n",
       "count  7.373390e+05  737339.000000  737339.000000  737339.000000  737339.0\n",
       "mean   1.526486e+09       6.935000      52.798143      52.798143       0.0\n",
       "std    7.342795e+05     151.224431     721.943659     721.943659       0.0\n",
       "min    1.525306e+09       0.000000       0.000000       0.000000       0.0\n",
       "25%    1.525723e+09       0.000000       1.000000       1.000000       0.0\n",
       "50%    1.526487e+09       1.000000       1.000000       1.000000       0.0\n",
       "75%    1.527045e+09       3.000000       4.000000       4.000000       0.0\n",
       "max    1.527725e+09   46164.000000  100389.000000  100389.000000       0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you first readin a dataset, I would recommend using `df.sample()` rather than `df.head()` because sometimes the first couple rows are fine, however, there might be missing values or mixed types in the column so it is better if you can get a big picture of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>403317</th>\n",
       "      <td>1525489748</td>\n",
       "      <td>Ice_Poseidon</td>\n",
       "      <td>lilpump6969</td>\n",
       "      <td>self.Ice_Poseidon</td>\n",
       "      <td>https://www.reddit.com/r/Ice_Poseidon/comments...</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>CAN WE JUST LEAVE ALREADY IM TIRED OF WATCHING...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612083</th>\n",
       "      <td>1527716193</td>\n",
       "      <td>FortNiteBR</td>\n",
       "      <td>X-IS-DEATH1</td>\n",
       "      <td>self.FortNiteBR</td>\n",
       "      <td>https://www.reddit.com/r/FortNiteBR/comments/8...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I managed to defy gravity with the shopping cart</td>\n",
       "      <td>I was just going up a hill and I started drivi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535475</th>\n",
       "      <td>1525667785</td>\n",
       "      <td>FortNiteBR</td>\n",
       "      <td>BroncoBoy48</td>\n",
       "      <td>self.FortNiteBR</td>\n",
       "      <td>https://www.reddit.com/r/FortNiteBR/comments/8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Suggestion] Why don’t they add refunding?</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177433</th>\n",
       "      <td>1526239390</td>\n",
       "      <td>ACCIDENTAL_HAIKU_BOT</td>\n",
       "      <td>ACCIDENTAL_HAIKU_BOT</td>\n",
       "      <td>self.ACCIDENTAL_HAIKU_BOT</td>\n",
       "      <td>https://www.reddit.com/r/ACCIDENTAL_HAIKU_BOT/...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/u/DarkFERDY's accidental haiku in /r/BABYMETAL</td>\n",
       "      <td>How do I get to \\n    the thread I kinda s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314754</th>\n",
       "      <td>1526459067</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>SANDRA254</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8j...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>So what do you guys do when it comes to findin...</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210107</th>\n",
       "      <td>1527440634</td>\n",
       "      <td>RocketLeagueExchange</td>\n",
       "      <td>WhatIsHam</td>\n",
       "      <td>self.RocketLeagueExchange</td>\n",
       "      <td>https://www.reddit.com/r/RocketLeagueExchange/...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[PC] [H] Striker White Lonewolf [W] Pricecheck...</td>\n",
       "      <td>add me or comment :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89940</th>\n",
       "      <td>1527099036</td>\n",
       "      <td>newsbotbot</td>\n",
       "      <td>-en-</td>\n",
       "      <td>mobile.twitter.com</td>\n",
       "      <td>https://mobile.twitter.com/FT/status/999351367...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@FT: Fed signals concern over trade tension ht...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694450</th>\n",
       "      <td>1526870824</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.Showerthoughts</td>\n",
       "      <td>https://www.reddit.com/r/Showerthoughts/commen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>So we’re all just still cool with referring to...</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25073</th>\n",
       "      <td>1526721329</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8k...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Instructors of Reddit, what is the worst case ...</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605025</th>\n",
       "      <td>1527012582</td>\n",
       "      <td>Ice_Poseidon</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.Ice_Poseidon</td>\n",
       "      <td>https://www.reddit.com/r/Ice_Poseidon/comments...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Mexican Andy can’t have Brandon on stream if h...</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc             subreddit                author  \\\n",
       "403317   1525489748          Ice_Poseidon           lilpump6969   \n",
       "612083   1527716193            FortNiteBR           X-IS-DEATH1   \n",
       "535475   1525667785            FortNiteBR           BroncoBoy48   \n",
       "177433   1526239390  ACCIDENTAL_HAIKU_BOT  ACCIDENTAL_HAIKU_BOT   \n",
       "314754   1526459067             AskReddit             SANDRA254   \n",
       "210107   1527440634  RocketLeagueExchange             WhatIsHam   \n",
       "89940    1527099036            newsbotbot                  -en-   \n",
       "694450   1526870824        Showerthoughts             [deleted]   \n",
       "25073    1526721329             AskReddit             [deleted]   \n",
       "605025   1527012582          Ice_Poseidon             [deleted]   \n",
       "\n",
       "                           domain  \\\n",
       "403317          self.Ice_Poseidon   \n",
       "612083            self.FortNiteBR   \n",
       "535475            self.FortNiteBR   \n",
       "177433  self.ACCIDENTAL_HAIKU_BOT   \n",
       "314754             self.AskReddit   \n",
       "210107  self.RocketLeagueExchange   \n",
       "89940          mobile.twitter.com   \n",
       "694450        self.Showerthoughts   \n",
       "25073              self.AskReddit   \n",
       "605025          self.Ice_Poseidon   \n",
       "\n",
       "                                                      url  num_comments  \\\n",
       "403317  https://www.reddit.com/r/Ice_Poseidon/comments...             2   \n",
       "612083  https://www.reddit.com/r/FortNiteBR/comments/8...             0   \n",
       "535475  https://www.reddit.com/r/FortNiteBR/comments/8...             0   \n",
       "177433  https://www.reddit.com/r/ACCIDENTAL_HAIKU_BOT/...             0   \n",
       "314754  https://www.reddit.com/r/AskReddit/comments/8j...             1   \n",
       "210107  https://www.reddit.com/r/RocketLeagueExchange/...             0   \n",
       "89940   https://mobile.twitter.com/FT/status/999351367...             0   \n",
       "694450  https://www.reddit.com/r/Showerthoughts/commen...             1   \n",
       "25073   https://www.reddit.com/r/AskReddit/comments/8k...             0   \n",
       "605025  https://www.reddit.com/r/Ice_Poseidon/comments...             3   \n",
       "\n",
       "        score  ups  downs                                              title  \\\n",
       "403317     13   13      0  CAN WE JUST LEAVE ALREADY IM TIRED OF WATCHING...   \n",
       "612083      0    0      0   I managed to defy gravity with the shopping cart   \n",
       "535475      1    1      0         [Suggestion] Why don’t they add refunding?   \n",
       "177433      1    1      0    /u/DarkFERDY's accidental haiku in /r/BABYMETAL   \n",
       "314754      1    1      0  So what do you guys do when it comes to findin...   \n",
       "210107      1    1      0  [PC] [H] Striker White Lonewolf [W] Pricecheck...   \n",
       "89940       1    1      0  @FT: Fed signals concern over trade tension ht...   \n",
       "694450      1    1      0  So we’re all just still cool with referring to...   \n",
       "25073       1    1      0  Instructors of Reddit, what is the worst case ...   \n",
       "605025      1    1      0  Mexican Andy can’t have Brandon on stream if h...   \n",
       "\n",
       "                                                 selftext  \n",
       "403317                                                NaN  \n",
       "612083  I was just going up a hill and I started drivi...  \n",
       "535475                                          [removed]  \n",
       "177433      How do I get to \\n    the thread I kinda s...  \n",
       "314754                                          [removed]  \n",
       "210107                               add me or comment :)  \n",
       "89940                                                 NaN  \n",
       "694450                                          [removed]  \n",
       "25073                                           [removed]  \n",
       "605025                                          [deleted]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `selftext` is the raw text of each Reddit post. But take a look at the column. There are missing values, `[deleted]`, `[removed]` which should not be considered as valid text.\n",
    "- We need to clean the text before we can further analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na with empty string\n",
    "df['selftext'] = df['selftext'].fillna('')\n",
    "# Replace `removed` and `deleted` with empty string\n",
    "tbr = ['[removed]', '[deleted]']\n",
    "df['selftext'] = df['selftext'].apply(lambda x: '' if x in tbr else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After cleansing the data, about 88% of our `selftext` column are just empty string.\n",
    "- It makes sense to concatenate the text with its title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8806152936437649\n"
     ]
    }
   ],
   "source": [
    "print(sum(df['selftext'] == '') / df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = df['title'] + ' ' + df['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145107</th>\n",
       "      <td>1526083203</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>HumanToes</td>\n",
       "      <td>self.Showerthoughts</td>\n",
       "      <td>https://www.reddit.com/r/Showerthoughts/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Your parents probably remember remember your c...</td>\n",
       "      <td>Your parents probably remember remember your c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182011</th>\n",
       "      <td>1526239388</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>CoolGuess</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8j...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If Facebook launches its own crypto currency, ...</td>\n",
       "      <td>If Facebook launches its own crypto currency, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613306</th>\n",
       "      <td>1527646507</td>\n",
       "      <td>ACCIDENTAL_HAIKU_BOT</td>\n",
       "      <td>ACCIDENTAL_HAIKU_BOT</td>\n",
       "      <td>self.ACCIDENTAL_HAIKU_BOT</td>\n",
       "      <td>https://www.reddit.com/r/ACCIDENTAL_HAIKU_BOT/...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/u/iobraska's accidental haiku in /r/Catholicism</td>\n",
       "      <td>/u/iobraska's accidental haiku in /r/Catholici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628921</th>\n",
       "      <td>1527694974</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>KrakNup</td>\n",
       "      <td>i.magaimg.net</td>\n",
       "      <td>http://i.magaimg.net/img/3f03.png</td>\n",
       "      <td>3</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>She has family connections to Bill Ayers (Weat...</td>\n",
       "      <td>She has family connections to Bill Ayers (Weat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537535</th>\n",
       "      <td>1525916069</td>\n",
       "      <td>FortNiteBR</td>\n",
       "      <td>BazookaBrawkler</td>\n",
       "      <td>self.FortNiteBR</td>\n",
       "      <td>https://www.reddit.com/r/FortNiteBR/comments/8...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Thanos fortnite</td>\n",
       "      <td>Thanos fortnite the new gamemode. fun but when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438965</th>\n",
       "      <td>1526785959</td>\n",
       "      <td>RocketLeagueExchange</td>\n",
       "      <td>SkyReveal</td>\n",
       "      <td>self.RocketLeagueExchange</td>\n",
       "      <td>https://www.reddit.com/r/RocketLeagueExchange/...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>☁️[Xbox][H] Black Veloce &amp;amp; Party Time [W] ...</td>\n",
       "      <td>☁️[Xbox][H] Black Veloce &amp;amp; Party Time [W] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411773</th>\n",
       "      <td>1525529734</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>reuters.com</td>\n",
       "      <td>https://www.reuters.com/article/us-australia-k...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Offbeat] - Carrot-addicted kangaroos hopping ...</td>\n",
       "      <td>[Offbeat] - Carrot-addicted kangaroos hopping ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447237</th>\n",
       "      <td>1526827070</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8k...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>What's the best advice you've ever received fo...</td>\n",
       "      <td>What's the best advice you've ever received fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142574</th>\n",
       "      <td>1526114645</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>garrusnogarrus</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8i...</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit, what tabs have you had open for a long...</td>\n",
       "      <td>Reddit, what tabs have you had open for a long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430534</th>\n",
       "      <td>1526782851</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>vulcan5301</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8k...</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>What cheap and inexpensive gifts can a broke, ...</td>\n",
       "      <td>What cheap and inexpensive gifts can a broke, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc             subreddit                author  \\\n",
       "145107   1526083203        Showerthoughts             HumanToes   \n",
       "182011   1526239388             AskReddit             CoolGuess   \n",
       "613306   1527646507  ACCIDENTAL_HAIKU_BOT  ACCIDENTAL_HAIKU_BOT   \n",
       "628921   1527694974            The_Donald               KrakNup   \n",
       "537535   1525916069            FortNiteBR       BazookaBrawkler   \n",
       "438965   1526785959  RocketLeagueExchange             SkyReveal   \n",
       "411773   1525529734         AutoNewspaper    AutoNewspaperAdmin   \n",
       "447237   1526827070             AskReddit             [deleted]   \n",
       "142574   1526114645             AskReddit        garrusnogarrus   \n",
       "430534   1526782851             AskReddit            vulcan5301   \n",
       "\n",
       "                           domain  \\\n",
       "145107        self.Showerthoughts   \n",
       "182011             self.AskReddit   \n",
       "613306  self.ACCIDENTAL_HAIKU_BOT   \n",
       "628921              i.magaimg.net   \n",
       "537535            self.FortNiteBR   \n",
       "438965  self.RocketLeagueExchange   \n",
       "411773                reuters.com   \n",
       "447237             self.AskReddit   \n",
       "142574             self.AskReddit   \n",
       "430534             self.AskReddit   \n",
       "\n",
       "                                                      url  num_comments  \\\n",
       "145107  https://www.reddit.com/r/Showerthoughts/commen...             0   \n",
       "182011  https://www.reddit.com/r/AskReddit/comments/8j...             4   \n",
       "613306  https://www.reddit.com/r/ACCIDENTAL_HAIKU_BOT/...             0   \n",
       "628921                  http://i.magaimg.net/img/3f03.png             3   \n",
       "537535  https://www.reddit.com/r/FortNiteBR/comments/8...             7   \n",
       "438965  https://www.reddit.com/r/RocketLeagueExchange/...             4   \n",
       "411773  https://www.reuters.com/article/us-australia-k...             0   \n",
       "447237  https://www.reddit.com/r/AskReddit/comments/8k...             2   \n",
       "142574  https://www.reddit.com/r/AskReddit/comments/8i...            14   \n",
       "430534  https://www.reddit.com/r/AskReddit/comments/8k...            23   \n",
       "\n",
       "        score  ups  downs                                              title  \\\n",
       "145107     10   10      0  Your parents probably remember remember your c...   \n",
       "182011      0    0      0  If Facebook launches its own crypto currency, ...   \n",
       "613306      1    1      0   /u/iobraska's accidental haiku in /r/Catholicism   \n",
       "628921    129  129      0  She has family connections to Bill Ayers (Weat...   \n",
       "537535      1    1      0                                    Thanos fortnite   \n",
       "438965      4    4      0  ☁️[Xbox][H] Black Veloce &amp; Party Time [W] ...   \n",
       "411773      1    1      0  [Offbeat] - Carrot-addicted kangaroos hopping ...   \n",
       "447237      2    2      0  What's the best advice you've ever received fo...   \n",
       "142574      2    2      0  Reddit, what tabs have you had open for a long...   \n",
       "430534      2    2      0  What cheap and inexpensive gifts can a broke, ...   \n",
       "\n",
       "                                                 selftext  \n",
       "145107  Your parents probably remember remember your c...  \n",
       "182011  If Facebook launches its own crypto currency, ...  \n",
       "613306  /u/iobraska's accidental haiku in /r/Catholici...  \n",
       "628921  She has family connections to Bill Ayers (Weat...  \n",
       "537535  Thanos fortnite the new gamemode. fun but when...  \n",
       "438965  ☁️[Xbox][H] Black Veloce &amp; Party Time [W] ...  \n",
       "411773  [Offbeat] - Carrot-addicted kangaroos hopping ...  \n",
       "447237  What's the best advice you've ever received fo...  \n",
       "142574  Reddit, what tabs have you had open for a long...  \n",
       "430534  What cheap and inexpensive gifts can a broke, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Convert all the text to lowercase - avoids having multiple copies of the same words.\n",
    "- Replace url in the text with empty space.\n",
    "- Replace all empty spaces with just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Convert all the string to lower cases\n",
    "df['selftext'] = df['selftext'].str.lower()\n",
    "# \\S+ means anything that is not an empty space\n",
    "df['selftext'] = df['selftext'].apply(lambda x: re.sub('http\\S*', '', x))\n",
    "# \\s+ means all empty space (\\n, \\r, \\t)\n",
    "df['selftext'] = df['selftext'].apply(lambda x: re.sub('\\s+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take a look at the dataframe after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>604207</th>\n",
       "      <td>1526955785</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>http://www.foxnews.com/politics/2018/05/21/hou...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>House Republicans to call for second special c...</td>\n",
       "      <td>house republicans call second special counsel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667500</th>\n",
       "      <td>1526914549</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>PanicAtTheMetro</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8l...</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>What are you excited for in the future?</td>\n",
       "      <td>what excited future?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545981</th>\n",
       "      <td>1525949838</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>NRDL</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8i...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>What's legitimately overrated in your experience?</td>\n",
       "      <td>what's legitimately overrated experience?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481297</th>\n",
       "      <td>1527364101</td>\n",
       "      <td>newsbotbot</td>\n",
       "      <td>-en-</td>\n",
       "      <td>mobile.twitter.com</td>\n",
       "      <td>https://mobile.twitter.com/BW/status/100046306...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@BW: Europe is ready to move on from Brexit ht...</td>\n",
       "      <td>@bw: europe ready move brexit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230012</th>\n",
       "      <td>1527439922</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8m...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Would you give your husband anal or anal oral ...</td>\n",
       "      <td>would give husband anal anal oral asked it? wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158158</th>\n",
       "      <td>1526088926</td>\n",
       "      <td>newsbotbot</td>\n",
       "      <td>-en-</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>https://twitter.com/APWestRegion/status/995108...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@AP: RT @APWestRegion: Parts of Hawaii's Big I...</td>\n",
       "      <td>@ap: rt @apwestregion: parts hawaii's big isla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297924</th>\n",
       "      <td>1526469265</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>miamiherald.com</td>\n",
       "      <td>http://www.miamiherald.com/news/business/artic...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[World] - Bank of England official apologizes ...</td>\n",
       "      <td>[world] - bank england official apologizes sex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517748</th>\n",
       "      <td>1525736710</td>\n",
       "      <td>Ice_Poseidon</td>\n",
       "      <td>maddogumadcunt</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://i.redd.it/h29c3fx1tiw01.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>&amp;lt;---- This many people think Scam Pepper sh...</td>\n",
       "      <td>&amp;lt;---- this many people think scam pepper ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411217</th>\n",
       "      <td>1525516941</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>https://www.nytimes.com/2018/05/05/style/steph...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Lifestyle] - The Man Who Bought New York | NY...</td>\n",
       "      <td>[lifestyle] - the man who bought new york | ny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299559</th>\n",
       "      <td>1526485582</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>rt.com</td>\n",
       "      <td>https://www.rt.com/usa/426907-trump-tower-meet...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[US] - Trump Tower meeting: Senate panel relea...</td>\n",
       "      <td>[us] - trump tower meeting: senate panel relea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc      subreddit              author              domain  \\\n",
       "604207   1526955785     The_Donald           [deleted]         foxnews.com   \n",
       "667500   1526914549      AskReddit     PanicAtTheMetro      self.AskReddit   \n",
       "545981   1525949838      AskReddit                NRDL      self.AskReddit   \n",
       "481297   1527364101     newsbotbot                -en-  mobile.twitter.com   \n",
       "230012   1527439922      AskReddit           [deleted]      self.AskReddit   \n",
       "158158   1526088926     newsbotbot                -en-         twitter.com   \n",
       "297924   1526469265  AutoNewspaper  AutoNewspaperAdmin     miamiherald.com   \n",
       "517748   1525736710   Ice_Poseidon      maddogumadcunt           i.redd.it   \n",
       "411217   1525516941  AutoNewspaper  AutoNewspaperAdmin         nytimes.com   \n",
       "299559   1526485582  AutoNewspaper  AutoNewspaperAdmin              rt.com   \n",
       "\n",
       "                                                      url  num_comments  \\\n",
       "604207  http://www.foxnews.com/politics/2018/05/21/hou...             3   \n",
       "667500  https://www.reddit.com/r/AskReddit/comments/8l...            16   \n",
       "545981  https://www.reddit.com/r/AskReddit/comments/8i...            21   \n",
       "481297  https://mobile.twitter.com/BW/status/100046306...             0   \n",
       "230012  https://www.reddit.com/r/AskReddit/comments/8m...             2   \n",
       "158158  https://twitter.com/APWestRegion/status/995108...             0   \n",
       "297924  http://www.miamiherald.com/news/business/artic...             0   \n",
       "517748                https://i.redd.it/h29c3fx1tiw01.jpg             3   \n",
       "411217  https://www.nytimes.com/2018/05/05/style/steph...             0   \n",
       "299559  https://www.rt.com/usa/426907-trump-tower-meet...             0   \n",
       "\n",
       "        score  ups  downs                                              title  \\\n",
       "604207     11   11      0  House Republicans to call for second special c...   \n",
       "667500      6    6      0            What are you excited for in the future?   \n",
       "545981      3    3      0  What's legitimately overrated in your experience?   \n",
       "481297      1    1      0  @BW: Europe is ready to move on from Brexit ht...   \n",
       "230012      0    0      0  Would you give your husband anal or anal oral ...   \n",
       "158158      1    1      0  @AP: RT @APWestRegion: Parts of Hawaii's Big I...   \n",
       "297924      1    1      0  [World] - Bank of England official apologizes ...   \n",
       "517748      7    7      0  &lt;---- This many people think Scam Pepper sh...   \n",
       "411217      1    1      0  [Lifestyle] - The Man Who Bought New York | NY...   \n",
       "299559      1    1      0  [US] - Trump Tower meeting: Senate panel relea...   \n",
       "\n",
       "                                                 selftext  \n",
       "604207  house republicans call second special counsel ...  \n",
       "667500                               what excited future?  \n",
       "545981          what's legitimately overrated experience?  \n",
       "481297                     @bw: europe ready move brexit   \n",
       "230012  would give husband anal anal oral asked it? wh...  \n",
       "158158  @ap: rt @apwestregion: parts hawaii's big isla...  \n",
       "297924  [world] - bank england official apologizes sex...  \n",
       "517748  &lt;---- this many people think scam pepper ba...  \n",
       "411217  [lifestyle] - the man who bought new york | ny...  \n",
       "299559  [us] - trump tower meeting: senate panel relea...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Steps and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before we start using machine learning methods on our text, there are some steps that we first want to perform so that our text is in a format that our model can interpret.\n",
    "- These steps include:\n",
    " - Filtering\n",
    " - Tokenization\n",
    " - Stemming\n",
    " - Lemmitization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first step is to remove punctuation, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = df['selftext'].apply(lambda x: re.sub('[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When examining a text, often there are words used within a sentence that holds no meaning for various data mining operations such as topic modeling or word frequency. \n",
    "    - Examples of this include \"the\", \"is\", etc. Collectively, these are known as \"stopwords\". \n",
    "- When mining for certain information, you should note whether your method should remove certain stopwords (for example, wordclouds). To illustrate an example, we will call upon the stopwords method from nltk. \n",
    "- Note, methods that interact with the text itself is usually found under nltk.corpus. Corpus is the linguistics term for set of structured text used for statistical study so be mindful of this specific vocabulary.\n",
    "- The stop words from nltk is just a Python list so you can easily append more stopwords to it. For example \"computer\" would be a stopword in corpus largely dealing with data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 selftext\n",
      "0                                                   meirl\n",
      "1            me_irl royal wedding going im browsing memes\n",
      "2                                                     irl\n",
      "3                                  irl haha lol post text\n",
      "4       whats go activity airport youre plane delayed ...\n",
      "5       anyone else stoked tonights battle bots whos f...\n",
      "6                                     reddit country suck\n",
      "7       redditors mental disabilities motivation go li...\n",
      "8       whats scariest paranormal experience ever scep...\n",
      "9       show called mildly interesting people would it...\n",
      "10      african americans reddit irritating funny stup...\n",
      "11      something common knowledge took embarrassingly...\n",
      "12      jobs version classic haha item scan guess must...\n",
      "13                                    think royals better\n",
      "14                         people reddit cynical bastards\n",
      "15      daily cosmetic sales 19 may weekly cosmetics a...\n",
      "16      idea cosmetc preview saw post sound test butto...\n",
      "17      april fools boogie bombs testing hop rocks thi...\n",
      "18      new ltm rainbow loot would slightly normal jum...\n",
      "19      anyone else get really high ping fortnite rece...\n",
      "20      sound enemy crouch walking hear anyone next th...\n",
      "21      tricera tomorrow tricera going 1 month old tom...\n",
      "22      another plea old music like ive stated want ol...\n",
      "23      epic xbox ps4 wondering add epic account xbox ...\n",
      "24      unlinking pcps4account split items know tons p...\n",
      "25      question solo showdown single competition acro...\n",
      "26      new idea build battle people dont know hard wo...\n",
      "27      increasing sensitivity max increase auto aim c...\n",
      "28                           epic get retarded ass emotes\n",
      "29      fortnite comp honor im little salty right game...\n",
      "...                                                   ...\n",
      "737309                              price check tw octane\n",
      "737310     pcp 4 triumph crates 4 velocity crates w 1 key\n",
      "737311                            h tw witch hat w offers\n",
      "737312  xbox h striker lime kalos lab w striker purple...\n",
      "737313         xbox h exotic dieci set w heat slip offers\n",
      "737314   h trigon tw voltaic lime nipper w heat chameleon\n",
      "737315                  xbox w pricecheck lime dune racer\n",
      "737316  xbox pricecheck got scammed nonexotic grey tun...\n",
      "737317                  h 46keys sb hypnotiks w tw octane\n",
      "737318                   h assorted crates w white steggo\n",
      "737319                               xbox h list w trades\n",
      "737320            ps4 h much w anything dont need anymore\n",
      "737321  h 8k 2k 1k 1k w party time saffron photon gree...\n",
      "737322  xbox h bmds w certified pinksaffronorange purp...\n",
      "737323             system h myitemsfortrade w offersiwant\n",
      "737324                                         discussion\n",
      "737325                                       h list w pcc\n",
      "737326  xbox h striker black invaders gksniper black v...\n",
      "737327                         trading crates rare wheels\n",
      "737328                      x box h pcc tw zomba w offers\n",
      "737329                                  xbox h list w pcc\n",
      "737330                xbox h list w pcc non painted hypno\n",
      "737331     xbox h nc striker tw draco w 4 heat small adds\n",
      "737332                xbox h list w pcc non painted hypno\n",
      "737333  h saffron octane amp saffron hikari p5 w uncom...\n",
      "737334                 system h list w toraspectrebiomass\n",
      "737335                xbox h list w black hexphase 1 heat\n",
      "737336             xbox h striker white nc draco w 4 heat\n",
      "737337  xbox h bmds w certified pink orange purple saf...\n",
      "737338                          xbox h tw octane w offers\n",
      "\n",
      "[737339 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df['selftext'] = df['selftext'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "print(df[['selftext']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization is the act of splitting text into a sequence of words. In this example, we will try a simplistic tokenization method below using the standard split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'toy', 'example.', 'Illustrate', 'this', 'example', 'below.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is a toy example. Illustrate this example below.\"\n",
    "sample_tokens = sample_text.split()\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Did you notice something? While we have the tokens, \"example\" and \"example.\" are treated as different tokens. As a NLP data scientist, you must make the choice on whether you choose to distinguish the two.\n",
    "\n",
    "- Note, various packages in Python such as the nltk package will default tokenize \".\" as a seperate token instead to designate it it's own special meaning. This can be illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ktread/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'toy',\n",
       " 'example',\n",
       " '.',\n",
       " 'Illustrate',\n",
       " 'this',\n",
       " 'example',\n",
       " 'below',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- However, textblob treats \".\" just as a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['This', 'is', 'a', 'toy', 'example', 'Illustrate', 'this', 'example', 'below'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "TextBlob(sample_text).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Various words in English have the same meaning. There are two main methods for handling tasks such as recognizing \"strike, striking, struck\" as the same words.\n",
    "\n",
    "- Stemming refers to the removal of suffixes, like “ing”, “ly”, “s”, etc. by a simple rule-based approach.\n",
    "\n",
    "- The most common stemming algorithms are:\n",
    " - [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/) (the older traditional method)\n",
    " - [Lancaster Stemmer](http://textanalysisonline.com/nltk-lancaster-stemmer) (a more aggressive modern stemmer)\n",
    "\n",
    "- Stemming and lemmatization can both be done with self written rules using creative forms of regex but for practical example demo in this notebook, we will implement the PorterStemmer method from nltk on the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonprocess_text = \"I am writing a Python string\"\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am write a python string\n"
     ]
    }
   ],
   "source": [
    "stemmed_text = ' '.join([stemmer.stem(word) for word in nonprocess_text.split()])\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: This is more robust than the standard regex implementation as we see here \"writing\" is converted to \"write\" but \"string\" isn't converted to \"stre\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike stemming, lemmatization will try to identify root words that are semantically similar to text based off a dictionary corpus. In essence, you can think of being able to replicate the effect manually by implementing a look-up method after parsing a text. Therefore, we usually prefer using lemmatization over stemming.\n",
    "\n",
    "- There are various dictionaries one can use to base lemmization off of. NLTK's [wordnet](http://wordnet.princeton.edu/) is quite powerful to handle most lemmatization task. We'll examine a few implementations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemztr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemztr.lemmatize('feet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note, lemmatization will return back the string if the text isn't found in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemztr.lemmatize('abacadabradoo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "\n",
    "- Unigrams do not usually contain as much information as compared to bigrams and trigrams. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. \n",
    "\n",
    "- The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.\n",
    "\n",
    "- Google hosts its n-gram corpora on [AWS S3](https://aws.amazon.com/datasets/google-books-ngrams/) for free. \n",
    "- The size of the file is about 2.2TB. You might consider using [Python API](https://github.com/dimazest/google-ngram-downloader)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams - Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(df['selftext'][5]).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can easily implement the N-gram function using native Python - it is a common nlp interview question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "find_ngrams(input_list, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=2000, width=800, height=400)\n",
    "# generate word cloud\n",
    "wc.generate(' '.join(df['selftext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This wordcloud is generated using all the text data. However, it makes more sense to have a separate wordcloud for each individual subreddit.\n",
    "- If you find any frequent word that doesn't contain useful information, you should consider adding it to your stopword list.\n",
    "- You can find more examples on the [documentation](http://amueller.github.io/word_cloud/auto_examples/index.html) and [blog post](http://minimaxir.com/2016/05/wordclouds/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment analysis refers to the use of natural language processing, text analysis, and computational linguistics to identify emotional states and subjective information.\n",
    "\n",
    "- Using sentiment analysis, we can gain information about the attitude of the speaker or writer of text with respect to the topic. \n",
    "\n",
    "- Today we will just call the [sentiment analysis API](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis) from TextBlob and take it like a black box as we haven't talked about machine learning yet.\n",
    "\n",
    "- We want to apply the function to the text column of the dataframe and generate two new columns called polarity and subjectivity. The process will take a long time so we will apply it to a sample of dataset.\n",
    "    - Polarity refers to the emotions expressed in the text.\n",
    "    - Subjectivity is a measure of how subjective vs objective the text is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's use sentiment analysis to analyze the relationship between polarity and number of thumb ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out all posts that have less than 100 upvotes\n",
    "sa_df = df.loc[df.ups > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "def sentiment_func(x):\n",
    "    sentiment = TextBlob(x['selftext'])\n",
    "    x['polarity'] = sentiment.polarity\n",
    "    x['subjectivity'] = sentiment.subjectivity\n",
    "    return x\n",
    "\n",
    "sample = sa_df.sample(sample_size).apply(sentiment_func, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.plot.scatter('ups', 'polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Resources:\n",
    "\n",
    "Other advanced libraries in Python that we will cover in the future lectures:\n",
    "\n",
    "[Spacy](https://spacy.io/)\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
