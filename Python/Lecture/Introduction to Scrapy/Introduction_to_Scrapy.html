<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Scrapy</title>
    <meta charset="utf-8">
    <meta name="author" content="NYC Data Science Academy" />
    <link rel="stylesheet" href="asset/css/footer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to Scrapy
### NYC Data Science Academy

---




# Outline
- Introduction to Scrapy

- An Example

  - Getting Started

  - items.py

  - wiki_spider.py

  - pipelines.py

  - settings.py

- In Class Lab

---
# Introduction to Scrapy

- A powerful application framework for extracting structured data from web pages

- You write rules to extract data and Scrapy does the rest

- Offers the capability to easily post-process and persist your scraped data

- Has a healthy community on [Github](https://github.com/scrapy/scrapy) and [Stack OverFlow](http://stackoverflow.com/tags/scrapy/info).

---
# Introduction to Scrapy

- You define **items** -- containers that are used to collect scraped data

- You define **spiders** -- Python classes which define how a certain site (or group of sites) will be scraped

- You define item **pipelines** -- Python classes that sequentially process data

- Then you deploy your spider!  

---
# An Example

- To introduce Scrapy, we'll go through the following tasks:
  
  - Defining items to extract
  
  - Writing a spider
  
  - Writing an item pipeline

- For this example, we'll scrape data from the academy award [wiki page](https://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films).

- You can find the example code in the `wiki` folder.

---
# An Example

![img](./asset/img/wiki_1.png)

---
# Getting Started

- First, we need to install the Scrapy package.

- For Mac users, open the terminal and execute the following command:
    

```bash
conda install scrapy
```

- For Windows users, open **Anaconda Prompt** and execute the following command:


```bash
conda install -c scrapinghub scrapy
```


---
# items.py
- Defines the containers that will store the scraped data

- You define the structure of the data you intend to scrape

- Once items are filled, they essentially act as Python dictionaries

![img](./asset/img/wiki_2.png)

---
# items.py
- Add the names of the fields we intend to scrape.  

- Items are declared using a simple class definition syntax, and scrapy Field objects.


```python
import scrapy
class WikiItem(scrapy.Item):
	film = scrapy.Field()
	year = scrapy.Field()
	awards = scrapy.Field()
	nominations = scrapy.Field()
```

- That's it!

---
# wiki_spider.py
- The file called **wiki_spider.py** is placed within the **wiki/spiders** directory. You need to create a file for each spider you use.

- A spider is a Python class that defines how the site will be scraped

  - How to perform the crawl (i.e., how to follow links, if necessary)
  
  - How to parse the contents of the page to extract Items

---
# wiki_spider.py
- A spider requires a few things (and must be a subclass of scrapy.Spider):

  - **name:** an attribute specifying a **unique** name to identify the spider
  
  - **start_urls:** an attribute listing the URLs the spider will start from
  
  - **allowed_urls:** the main domain of the website you want to scrape
  
  - **parse():** a method of the spider responsible for processing a Response object downloaded from the URL and returning scraped data (as well as more URLs to follow, if necessary)


---
# wiki_spider.py
- First, we will need to import `scrapy.Spider` (from which our spider will inherit), and the item class we defined in **items.py**:

```python
from scrapy import Spider
from wiki.items import WikiItem
```

- Next, we define our spider class, and give it its required attributes:

```python
from scrapy import Spider
from wiki.items import WikiItem
class WikiSpider(Spider):
	name = 'wiki_spider'
	allowed_urls = ['https://en.wikipedia.org']
	start_urls = ['https://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films']
```

---
# wiki_spider.py
- Now, we need to define the Spider's parse method.  

- The parse method is responsible for:
  - Parsing the responses (web pages) generated by Request objects (for each of the URLs in the start_urls list)

  - Returning more Request objects (if your spider is following links), or iterables of these

  - Yielding scraped data for processing

- Our parse method will simply extract the film title, year, awards, and nominations.  

---
# wiki_spider.py
- In Scrapy, you can parse a page's content and extract data from the HTML source code using the built-in **Selector** class.

  - These are called selectors because they "select" certain parts of the HTML document specified by either XPath or CSS expressions

- We'll fill our items using the built-in Selector along with XPath expressions.

  - For more information about selectors and other extraction mechanisms, see the [documentation](https://doc.scrapy.org/en/latest/topics/selectors.html) of Selectors.

---
# XPath
- But what is XPath, and how do we use it?

- XPath is a **language** for selecting tags in XML or HTML documents.

- Single slash and double slash are most commonly seen symbols in xpath

  - Single slash `/` means a **direct** child of the current tag
  
  - Double slash `//` means **any descendant** tag of the current tag in the html tree which feets the locator.
  

  
- Dot `.` refers to the current tag. 

- For example:
  - `/html/head/title` selects the `&lt;title&gt;` tag inside a `&lt;head&gt;` tag of an HTML tag.
  
  - `//tr` selects all the `&lt;tr&gt;` tags from the html tag.
  
  - `./td[2]` select the **second** `td` tag after the current tag XPath start counting from 1 rather than 0.
  
---
# XPath
- `@` find the tag with certain attribute(s)
  
  - `//div[@class="mine"]` selects all the `&lt;div&gt;` tags having an attribute `class="mine"`

  - `//div[@itemprop="some_prop" and @class="some_class"]` selects all the `&lt;div&gt;` tags having both attributes

- `*` will find any type of tag that match the xpath

  - `//*[@class="mine"]` select all the tags including `&lt;div&gt;`, `&lt;p&gt;`...

- To select the text of a tag, add `/text()` at the end of the expression
  
  - `//div[@class="mine"]/text()`

- To select any attribute of a tag, add `/@name_of_attribute` at the end of the expression

  - `//div[@class="mine"]/@itemprop`

---
# XPath
- Luckily, Chrome (and other browsers) have a developer's tool for inspecting the structure of web pages.

&lt;img src="./asset/img/wiki_3.png" height="450px" width="550px", align="middle"&gt;


---
# XPath
&lt;img src="./asset/img/wiki_4.png" height="450px" width="600px", align="middle"&gt;


---
# XPath
- It appears that each row of this table constitutes a `&lt;tr&gt;` tag inside of the `&lt;table class=...&gt;` tag

- We can extract all of these `&lt;tr&gt;` tags with the XPath expression `//path/to/table/tbody/tr`

- So, how can we determine the XPath of the `&lt;table&gt;` tag?

- Again, Chrome developer tools to the rescue!

---
# XPath
&lt;img src="./asset/img/wiki_5.png" height="450px" width="600px", align="middle"&gt;

---
# XPath
- The path to the table of Academy Award winning films is:

  - `//*[@id="mw-content-text"]/div/table`
  
  - This means "select the table inside the `div` tag with `id="mw-content-text"`"
  
- We now want to extract all the `&lt;tr&gt;` tags starting from this xpath
    
    - Remember there is a `tbody` tag after the `table` tag.
    
    ```//*[@id="mw-content-text"]/div/table/tbody/tr
    ```

---
# XPath
- To check the xpath (and for general debugging), we will use a tool called scrapy shell.

- You need to have **two terminals/Anaconda Prompt** opened at the same time, one is for debugging using scrapy shell (doesn't matter where you start it) and the other one you can cd to the project directory and deploy the spider later.


```bash
scrapy shell "url_of_the_page_you're_scraping"
```

- Note: the quotation mark is **mandatory** because scrapy shell might interpret part of your url (most likely with a '-') as its argument.

- This will start a **Python shell** with the Scrapy package imported, along with a [response](https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.HtmlResponse) object from the request to the supplied url.

- Make sure you see the `(200)` [status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) on the terminal.
---
# Scrapy Shell
- To extract the `&lt;tr&gt;` tags from the page, type:


```python
response.xpath('//*[@id="mw-content-text"]/div/table/tbody/tr')
```

- This returns a **SelectorList** object. Let's save it as a variable called `rows`.

```python
rows=response.xpath('//*[@id="mw-content-text"]/div/table/tbody/tr')
```

- We can check the type and len of `rows`.

```python
print(type(rows))
print(len(rows))
```


---
# Scrapy Shell
- Now we have a SelectorList, each element in the list being the `&lt;tr&gt;` tag that comprises a row of the table.

- The next task is to iterate over this list, and extract the fields to fill our items.  

- For each of these `&lt;tr&gt;` tags, we need to extract:
  - The text from the `&lt;a&gt;` tags within the first and second `&lt;td&gt;` tags (these are the film title and year, respectively)
  
  - The text from the third and fourth `&lt;td&gt;` tags (these are awards and nominations, respectively)

- To test the xpath for those `td` tags, we just pick one of the rows randomly.

```python
row = rows[2]
```


---
# Scrapy Shell
- We have already found the rows in the table, so we can use **relative path** if we want to go one level deeper.

- `extract()` will return a **Python list** that contains all the values that match the xpath. Each variable here is just a list of one single string. 



```python
# Get the movie title
film = row.xpath('./td[1]/i/a/text()').extract()
# Get the release year
year = row.xpath('./td[2]/a/text()').extract()
# Get the number of awards
awards = row.xpath('./td[3]/text()').extract()
# Get the number of nominations
nominations = row.xpath('./td[4]/text()').extract()
```

---
# Scrapy Shell
- You only want to save the value to the local file instead of the list. So you might want come up with code like the following:


```python
# Get the movie title
film = row.xpath('./td[1]/i/a/text()').extract()[0]
```

- The code will break if your xpath is wrong because the list will be empty.

- `extract_first()` will return the **first element** from the list. It will return None if the list is empty.


```python
# Get the movie title
film = row.xpath('./td[1]/i/a/text()').extract_first()
```

- If you want to extract some other elements instead of the first one, then you need to use **try/except** statement in order to handle the exceptions.

---
# wiki_spider.py
- If we double check the website, some of the film titles they are in bold and with yellow background. So the xpath will fail for those movies.

- We can pre-define couple patterns and save them in a list.

- It makes sense to put the most commonly seen xpath as the first one.

- If the name of the film is missing, we would just skip the current one.


```python
patterns = ['./td[1]/i/a/text()', './td[1]/i/b/a/text()',\
            './td[1]/i/span[2]//text()', './td[1]/i/b/span/text()']
for pattern in patterns:
	# extract_first() will return the first element from the list
	# Return an empty string if the list is empty.
    film = row.xpath(pattern).extract_first()
    if film:
        break
if not film:
    continue
```

---
# wiki_spider.py
- Then you fill an item with:

```python
item = WikiItem()
item['film'] = film
item['year'] = year
item['awards'] = awards
item['nominations'] = nominations
```

- Finally, yield this item to the item pipeline that we will introduce next:

```python
yield item
```

---
# pipelines.py
- Once an item has been scraped, it wll be sent to the item pipeline

- Typical uses of an item pipeline are:

  - Cleansing HTML data
  
  - Validating scraped data
  
  - Checking for duplicates (and dropping them)
  
  - Saving the scraped items to local file or database

---
# pipelines.py
- What is the last thing you want to see when you import a dataset? Lots of missing value!

- Definitely you don't want to cause headache for yourself! Before we save those items to a csv file, it is always a good practice to check if all the keys have values.

- **Note:** This pipeline is **OPTIONAL**. If you are fine with missing values, then you should exclude it from your pipeline.

- Each item pipeline component is a Python class that must implement a method with signature `process_item(self, item, spider)`.


```python
from scrapy.exceptions import DropItem
from scrapy.exporters import CsvItemExporter
class ValidateItemPipeline(object):
    def process_item(self, item, spider):
        if not all(item.values()):
            raise DropItem("Missing values!")
        else:
            return item
```

---
# pipelines.py
- You can see there are commas inside movie titles, which makes it very hard to construct each line manually. 

- We will use the CsvItemExporter from [feed exports](https://doc.scrapy.org/en/latest/topics/feed-exports.html).



```python
class WriteItemPipeline(object):
    def __init__(self):
        self.filename = 'academy_awards.csv'
    def open_spider(self, spider):
        self.csvfile = open(self.filename, 'wb')
        self.exporter = CsvItemExporter(self.csvfile)
        self.exporter.start_exporting()
    def close_spider(self, spider):
        self.exporter.finish_exporting()
        self.csvfile.close()
    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item
```
 
---
# pipelines.py
- You can see more examples of item pipelines [here](https://doc.scrapy.org/en/latest/topics/item-pipeline.html#item-pipeline-example).

- Also, check out [feed exports](https://doc.scrapy.org/en/latest/topics/feed-exports.html), which allow you to serialize the scraped data in a number of formats including a csv file.

- The best example to understand what pipeline is:
&lt;center&gt;
&lt;img src="https://media.giphy.com/media/Sbxnepl8SSqWs/giphy.gif" width="400" height="350"/&gt;
&lt;/center&gt;
---
# settings.py
- The last file to edit is **settings.py**

- This is where you designate the behavior of all scrapy components (including item pipelines, spiders themselves, and more general global settings)

- Since we defined two `PipelineItems` in **pipelines.py**, so we need to give each of them a priority number in **settings.py**.

- We want to validate our item first before we save it to the csv file, so that's why here we give validate pipeline a smaller number, which stands for higher priority.


```python
ITEM_PIPELINES = {'wiki.pipelines.ValidateItemPipeline': 100, 
					'wiki.pipelines.WriteItemPipeline': 200}
```

---
# Deploy wiki_spider
- Now we're ready to deploy our spider!

- This is done with the following command (assuming you're in the **top** level of the project's directory, the one contains scrapy.cfg file):


```bash
scrapy crawl wiki_spider
```

- You should see stuff like this:

&lt;img src="./asset/img/wiki_6.png" align="middle"&gt;

- Presee `Ctrl+C` if you want to terminate the process.

---
# In Class Lab
- We scraped the Academy Award winners from wiki page. Suppose in the web scraping project, we want to find out is there any relationship between the number of nominations with the movie budgets.

- We can scrape the information we want from [the-numbers](https://www.the-numbers.com/movie/budgets/all) website:

&lt;img src="./asset/img/lab_1.png" height="350px" width="650px", align="middle"&gt;

---
# In Class Lab
- The solution can be found in the `lab_solution` folder but we will start from scratch.

- First, open a new terminal and cd to the folder you want to create the scrapy project. Then run the following command:


```bash
scrapy startproject budget
```

- You will find a new folder called budget with the following structure.

```{}
scrapy.cfg         
budget/      
  items.py
  middlewares.py
  pipelines.py  
  settings.py
  spiders/
```

---
# items.py
- Let's start with **items.py**. Remember you need to define the structure of the data you intend to scrape in items.py.

- We want to scrape the following information:
  
  - Release Date
  
  - Movie Title

  - Production Budget

  - Domestic Gross

  - Worldwide Gross

&lt;img src="./asset/img/lab_2.png" align="middle"&gt;


---
# items.py
- Here we defined five fields of our BudgetItem and call them `RDate`, `Title`, `PBudget`, `DomesticG`, `WorldwideG` separately. 

- You can name them differently, but remember they serve as the keys to your `BudegtItem` if you take each item as a dictionary.


```python
import scrapy
class BudgetItem(scrapy.Item):
    # define the fields for your item here like:
    RDate = scrapy.Field()
    Title = scrapy.Field()
    PBudget = scrapy.Field()
    DomesticG = scrapy.Field()
    WorldwideG = scrapy.Field()
```

---
# budget_spider.py
- Create a file called **budget_spider.py** under **budget/spiders/**

- It doesn't matter how you name it, but this is where you put your spider file and spend most of your time debugging.

- Your folder structure should look like this after this step.

```{}
scrapy.cfg         
budget/      
  items.py
  middlewares.py
  pipelines.py  
  settings.py
  spiders/
*    budget_spider.py
```

---
# budget_spider.py
- Now comes the hardest part. So let's try to figure it out step by step.

- Definitely the first thing to do is to import the module and the budget item we defined in items.py.


```python
from scrapy import Spider
from budget.items import BudgetItem
```

---
# budget_spider.py
- Next we need to define our own Spider, here we call it `BudgetSpider`

- We give the spider a name called `budget_spider`. Later on when we run it on the terminal, we just call:

```bash
scrapy crawl budget_spider
```

- Don't forget to specify allowed_urls and start_urls!


```python
class BudgetSpider(Spider):
    name = "budget_spider"
    allowed_urls = ['https://www.the-numbers.com/']
    start_urls = ['https://www.the-numbers.com/movie/budgets/all']
    
    def parse(self, response):
		    pass
```

---
# budget_spider.py
- Next step would be finding the xpath of those rows that we want to scrape and put it in the parse function. The best way to debug the XPath is using scrapy shell.

- Open a new terminal/Anaconda Prompt and type:

```bash
scrapy shell "https://www.the-numbers.com/movie/budgets/all"
```

- Then you get a Python shell and get a [html response](https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.HtmlResponse) from the url called response.

- However, you will see the status code `(403)` rather than `(200)` from the logs on the terminal. `(403)` means your request is forbidden :(
---
# budget_spider.py
- We need to add `USE_AGENT` to our request. Quit the current scrapy shell session first by `Ctrl+d` and then type the following command.


```bash
scrapy shell -s USER_AGENT="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36" "https://www.the-numbers.com/movie/budgets/all"
```

- You can find the `USER_AGENT` from the network tab on Chrome or just pick a random one from [here](https://techblog.willshouse.com/2012/01/03/most-common-user-agents/).

- First we want to locate the table that contains all the rows with the help of Google Chrome.


```python
rows = response.xpath('//*[@id="page_filling_chart"]/center/table/tbody/tr')
```

- Now we have a SelectorList of all the &lt;tr&gt; tags called rows. Let's check out the length of it first.


```python
len(rows)
```

---
# budget_spider.py
- Looks like there is nothing there. Maybe something is wrong with our xpath?

- This is because `tbody` tag is [optional](https://stackoverflow.com/a/34130653). It is not included in the html source code but rather created by Chrome.

- A more robust solution might be:

```python
rows = response.xpath('//*[@id="page_filling_chart"]/center/table//tr')
```

- Then print out the length of the SelectorList again.

```python
len(rows)
```

---
# budget_spider.py
- Next step, we need to find the specific xpath of each column.

- Remember we already found all the rows of the table, so we can use **relative path**.

- Release Date:

```python
rows[1].xpath('./td[2]/a/text()').extract_first()
```

- Movie Title:

```python
rows[1].xpath('./td[3]/b/a/text()').extract_first()
```

- Let's see whether you can figure out the xpath for the other three columns.

---
# budget_spider.py
- Once we get the xpath of each column from scrapy shell, we can copy and paste them to the `parse()` function in **budget_spider.py**

- We iterate through all the rows on the page and for each row, we have `RDate`, `Title`, `PBudget`, `DomesticG`, `WorldwideG`


```python
for row in rows:
    RDate = row.xpath('./td[2]/a/text()').extract_first()
    Title = row.xpath('./td[3]/b/a/text()').extract_first()
    PBudget = row.xpath('./td[4]/text()').extract_first()
    DomesticG = row.xpath('./td[5]/text()').extract_first()
    WorldwideG = row.xpath('./td[6]/text()').extract_first()
```

---
# budget_spider.py
- We create an empty BudgetItem object for each movie and assign the value to each corresponding key.

- At the end of each iteration, we yield the item.


```python
for row in rows:
    RDate = row.xpath('./td[2]/a/text()').extract_first()
    Title = row.xpath('./td[3]/b/a/text()').extract_first()
    PBudget = row.xpath('./td[4]/text()').extract_first()
    DomesticG = row.xpath('./td[5]/text()').extract_first()
    WorldwideG = row.xpath('./td[6]/text()').extract_first()
    
    item = BudgetItem()
    item['RDate'] = RDate
    item['Title'] = Title
    item['PBudget'] = PBudget
    item['DomesticG'] = DomesticG
    item['WorldwideG'] = WorldwideG
    
    yield item
```

---
# pipelines.py
- Good job! We are more than half way through the whole project. The only files left are **pipelines.py** and **settings.py**. Let's start with **pipelines.py** first.

- Remember we need to tell scrapy how to handle the BudgetItem after we yield it from the parse function. We can simply copy and paste the pipelines we used in the wiki example by replacing the name of the csv file.

- **Note**: pipeline classes are pretty generic, which means you can reuse them in another project. However, **NEVER** reuse your scrapy project.

---
# pipelines.py
- **Note:** This pipeline is **OPTIONAL**. If you are fine with missing values, then you should exclude it from your pipeline.


```python
from scrapy.exceptions import DropItem
from scrapy.exporters import CsvItemExporter
class ValidateItemPipeline(object):
    def process_item(self, item, spider):
        if not all(item.values()):
            raise DropItem("Missing values!")
        else:
            return item
```

---
# pipelines.py
- We will use the CsvItemExporter from [feed exports](https://doc.scrapy.org/en/latest/topics/feed-exports.html) to write items.


```python
class WriteItemPipeline(object):
    def __init__(self):
        self.filename = 'budget.csv'
    def open_spider(self, spider):
        self.csvfile = open(self.filename, 'wb')
        self.exporter = CsvItemExporter(self.csvfile)
        self.exporter.start_exporting()
    def close_spider(self, spider):
        self.exporter.finish_exporting()
        self.csvfile.close()
    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item
```

---
# settings.py
- Since we defined two `PipelineItems` in **pipelines.py**, so we need to give each of them a priority number in **settings.py**.

- We want to validate our item first before we save it to the csv file, so that's why here we give validate pipeline a smaller number, which stands for higher priority.


```python
ITEM_PIPELINES = {'budget.pipelines.ValidateItemPipeline': 100, 
					'budget.pipelines.WriteItemPipeline': 200}
					
ROBOTSTXT_OBEY = False
DOWNLOAD_DELAY = 1
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'
```

---
# Deploy Spider
- Now we're ready to deploy our spider!

- First make sure you cd to the top of your project directory (where you have the scrapy.cfg)


```bash
*budegt
  scrapy.cfg         
  budget/      
    items.py
    middlewares.py
    pipelines.py  
    settings.py
    spiders/
      budget_spider.py
```

- Then execute the following line of command.

```bash
scrapy crawl budget_spider
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('showSlide', function (slide) {setTimeout(function() {window.dispatchEvent(new Event('resize'));}, 100)});</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js"></script>
<script>
(function () {
  $('.remark-slide-content').prepend('<div class="nyc-header" />');
})();
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
